{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNnGK00hzMPH"
   },
   "source": [
    "# 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdDTfSyhzcmC"
   },
   "source": [
    "在这个Notebook里面，我将尝试利用现有技术实现一个基于Transformer的入侵检测系统并尽可能对其进行改进。首先，我将描述本架构的各个组成部分，在阐明其原理后对模型进行训练与评估，并在最后在相同问题下将该架构与其他架构进行对比。\n",
    "\n",
    "以下是为训练此模型而导入的包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Hgk3u3QaGR1w"
   },
   "outputs": [],
   "source": [
    "# !pip install einops\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install scikit-learn\n",
    "# !pip install imbalanced-learn\n",
    "# !pip install scikit-image\n",
    "# !pip install torchsummary\n",
    "# !pip install rtdl_num_embeddings\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9tBNq4k6B0L"
   },
   "source": [
    "# 结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpbc5i3s6ODr"
   },
   "source": [
    "## 嵌入(Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBQYPZ7u6T3a"
   },
   "source": [
    "在该部分里，数值信息将被转换为可被Transformer识别的特征值并提供给后面的结构进行学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKXbzP30kOED"
   },
   "source": [
    "$ReLU(Linear(Periodic(x_i)))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sIPm68LQ8iiT"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, dim, num_numerical_types=25):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(num_numerical_types, dim))\n",
    "        self.biases = nn.Parameter(torch.randn(num_numerical_types, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b n -> b n 1')\n",
    "        return x * self.weights + self.biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IH0RsbBkK5I"
   },
   "source": [
    "## 位置编码(Positional Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nf4ErG_gkyb_"
   },
   "source": [
    "由于嵌入产生的信息并不带有位置信息，故需要在学习前将位置信息写入训练数据中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYa6t4yQnmqN"
   },
   "source": [
    "$$PE(pos, 2i) = \\sin(pos/1000^{2i/d\\_model})$$\n",
    "$$PE(pos, 2i+1) = \\cos(pos/1000^{2i/d\\_model})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QPnc3zj8nqbp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from math import cos, sin, sqrt\n",
    "\n",
    "class Positional_Encoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 25):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0,d_model,2):\n",
    "                pe[pos,i] = sin(pos / (1000**(i/d_model)))\n",
    "                pe[pos,i+1] = cos(pos / (1000**(i/d_model)))\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\",pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * sqrt(self.d_model)\n",
    "        seq_len = x.size(1)\n",
    "        y = Variable(self.pe[:,:seq_len],requires_grad=False).cuda()\n",
    "        x = x + y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnqY-Q7en1jI"
   },
   "source": [
    "## 注意力机制(Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4x1Clko8oK4g"
   },
   "source": [
    "本部分用于对输入向量进行处理以获取输入与输出间的对应关系。具体计算过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2U4rFViWoMPy"
   },
   "source": [
    "$$Attention(Q,K,V) = softmax\\left(M+\\frac{QK^T}{\\sqrt{d_k}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CPYaAH59oTj-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "      shape = scores.shape\n",
    "      mask = get_mask(shape[0], 8, 10)\n",
    "      scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "\n",
    "    output = torch.matmul(scores, v)\n",
    "    return output\n",
    "\n",
    "class Multi_Head_Attention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        bs = q.size(0)\n",
    "\n",
    "        # perform linear operation and split into h heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "\n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
    "\n",
    "        output = self.out(concat)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDdttxGBejsR"
   },
   "source": [
    "## 掩蔽(Masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pa9RVFRufU_T"
   },
   "source": [
    "该部分用于对输入解码器(Decoder)的向量进行掩蔽来使得解码器能够根据上下文来推算结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GVhnYainfUSo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def get_mask(batch_size, heads, seq_size):\n",
    "    mask_prob = 0.2\n",
    "    mask = torch.rand((batch_size, heads, seq_size, seq_size)) > mask_prob\n",
    "    return mask.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d4tpIRUiXpe"
   },
   "source": [
    "## 前馈网络(Feed-Forward Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTW1sRE6ijFy"
   },
   "source": [
    "该部分主要用于记忆注意力机制计算所产生的的关系。具体原理如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0mGUiJviuBc"
   },
   "source": [
    "$$FFN(x) = \\max(0,xW_1 + b_1)W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_WtVjTp2iw-Z"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=1024, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ez3GmV8qlEoz"
   },
   "source": [
    "## 层归一化(Layer Norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pf8abbIElJZp"
   },
   "source": [
    "主要用于使不同范围的数据归一化到[0, 1]区间内，方便模型进行处理，具体原理如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$LN(x) = \\frac{x - \\mu}{\\delta}\\cdot \\alpha + \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.size = d_model\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxD3yhUnYwG_"
   },
   "source": [
    "## 编码器(Encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKUKjwm0i2BI"
   },
   "source": [
    "整个编码器由数个编码器嵌入器、数个编码器层与层归一化层组成。其中，一个编码器层由层归一化、注意力机制与前馈神经网络层组成，数据在经过这些处理后由dropout方法进行输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class Encoder_Layer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = Multi_Head_Attention(heads, d_model, dropout)\n",
    "        self.feedf = FeedForward(d_model).cuda()\n",
    "        self.dropout_1 = nn.Dropout(dropout).cuda()\n",
    "        self.dropout_2 = nn.Dropout(dropout).cuda()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.feedf(x2))\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, N, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(d_model)\n",
    "        self.pe = Positional_Encoder(d_model)\n",
    "        self.layers = get_clones(Encoder_Layer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "\n",
    "    def forward(self, src, mask = None):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x,mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-OdE8wWZJjO"
   },
   "source": [
    "## 解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9pA9SiVx0BFB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class Decoder_Layer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        self.attn = Multi_Head_Attention(heads, d_model, dropout)\n",
    "        self.msk_attn = Multi_Head_Attention(heads, d_model, dropout)\n",
    "        self.feedf = FeedForward(d_model).cuda()\n",
    "        self.dropout_1 = nn.Dropout(dropout).cuda()\n",
    "        self.dropout_2 = nn.Dropout(dropout).cuda()\n",
    "        self.dropout_3 = nn.Dropout(dropout).cuda()\n",
    "\n",
    "    def forward(self, x, e_outputs, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.msk_attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.attn(x2,e_outputs,e_outputs,mask=None))\n",
    "        x2 = self.norm_3(x)\n",
    "        x = x +self.dropout_3(self.feedf(x2))\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, N, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(d_model)\n",
    "        self.pe = Positional_Encoder(d_model,1)\n",
    "        self.layers = get_clones(Decoder_Layer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "\n",
    "    def forward(self, trg, e_outputs, mask = None):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x,e_outputs,mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LH1P_Gj4ZNxo"
   },
   "source": [
    "## 整体架构定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S47cD2sk0Uoa"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zEg0bhTe0VD2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, trg_vocab, d_model, N, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, N , heads, dropout)\n",
    "        self.decoder = Decoder(d_model, N , heads, dropout)\n",
    "        self.out = nn.Linear(25*d_model, trg_vocab)\n",
    "\n",
    "    def forward(self, src, trg_mask=None):\n",
    "        e_outputs = self.encoder(src, None)\n",
    "        d_output = self.decoder(src, e_outputs, trg_mask)\n",
    "        d_intermediate = d_output.view(d_output.size(0), -1)\n",
    "        output = self.out(d_intermediate)\n",
    "        output = torch.softmax(output,dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TdXPFHO1zPv"
   },
   "source": [
    "# 数据集与数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoE5dRpO1-PU"
   },
   "source": [
    "## NF-CEC-CIC-IDS2018-v2\n",
    "下载数据集:https://rdm.uq.edu.au/files/ce5161d0-ef9c-11ed-827d-e762de186848\n",
    "\n",
    "昆士兰大学Sarhan团队利用了CSE-CIC-IDS2018数据集的原始pcap文件生成了一个基于NetFlow的数据集，称为NF-CSE-CIC-IDS2018-v2。总的流量数量为18,893,708，其中2,258,141（11.95%）是攻击样本，16,635,567（88.05%）是良性样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDLur9Kj2E4o"
   },
   "source": [
    "## 数据处理\n",
    "本部分用于对数据进行预处理。由于本数据集的特殊性——正常流量远多于异常攻击流量，因此需要对数据进行下采样与SMOTE处理，前者用于限制数量较多的样本的数量，后者用于对数量较少的样本进行插值处理以增加其数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Y_u4bTpXFOr1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "\n",
    "# 声明全局变量\n",
    "directory_path = 'SIDD/'\n",
    "\n",
    "\n",
    "# 定义数据集类\n",
    "class CICIDSDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.features = data[:, :-3]  # 特征列\n",
    "        self.at_type = data[:, -3]  # 攻击类型列\n",
    "        self.labels = data[:, -2:]  # 1 Hot Encoded Label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = torch.from_numpy(self.features[idx]).float()\n",
    "        label = self.labels[idx]\n",
    "        at_type = self.at_type[idx]\n",
    "\n",
    "        return feature, label, at_type\n",
    "\n",
    "\n",
    "def data_process(data, label_mapping):\n",
    "    # Undersampling & SMOTE\n",
    "    max_class_size = 100000  # Size of all Classes for Undersampling\n",
    "    class_counts = data['attack_type'].value_counts()\n",
    "    classes_to_undersample = class_counts[class_counts > max_class_size]\n",
    "\n",
    "    under_sampler = RandomUnderSampler(sampling_strategy={\n",
    "        label: 7 * max_class_size if label_mapping[\n",
    "                                         label] == \"Benign\" else max_class_size if label in classes_to_undersample else\n",
    "        class_counts[label] for label in np.unique(data['attack_type'])\n",
    "    }, random_state=42)\n",
    "    nn_estimator = NearestNeighbors(n_neighbors=5, n_jobs=-1)\n",
    "    smote = SMOTE(sampling_strategy={\n",
    "        label: 7 * max_class_size if label_mapping[label] == \"Benign\" else max_class_size for label in\n",
    "        np.unique(data['attack_type'])\n",
    "    }, k_neighbors=nn_estimator, random_state=42)\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    features = data.drop(columns=['attack_type', 'labels'])\n",
    "    labels = data['attack_type']\n",
    "\n",
    "    print('Under sampling...')\n",
    "    sampled_features, sampled_labels = under_sampler.fit_resample(features, labels)\n",
    "    print('Using SMOTE...')\n",
    "    balanced_features, balanced_labels = smote.fit_resample(sampled_features, sampled_labels)\n",
    "    print('Scaling...')\n",
    "    scaled_data = scaler.fit_transform(balanced_features)\n",
    "\n",
    "    data = pd.DataFrame(data=scaled_data, columns=features.columns)\n",
    "    data = pd.concat([data, balanced_labels], axis=1)\n",
    "\n",
    "    # 1 Hot Encoding\n",
    "    data['Attack_Label'] = data['attack_type'].apply(lambda x: \"Attack\" if label_mapping[x] != \"Benign\" else \"Benign\")\n",
    "    print('Label encoding...')\n",
    "    encoded_labels = pd.get_dummies(data['Attack_Label'], prefix='', prefix_sep='')\n",
    "    data = pd.concat([data, encoded_labels], axis=1)\n",
    "    data.drop('Attack_Label', axis=1, inplace=True)\n",
    "\n",
    "    label_counts = data['attack_type'].value_counts()\n",
    "    print(label_counts)\n",
    "    return data\n",
    "\n",
    "\n",
    "def pca(feature, label, target_variance=0.95):\n",
    "    print('Using PCA...')\n",
    "    scaler = StandardScaler()\n",
    "    feature = np.nan_to_num(scaler.fit_transform(feature))\n",
    "    Iter = PCA(svd_solver='randomized')\n",
    "    feature = Iter.fit_transform(feature)\n",
    "    explained_variance_ratio = Iter.explained_variance_ratio_\n",
    "    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "    n_components = np.argmax(cumulative_variance_ratio >= target_variance) + 1\n",
    "    Iter = PCA(n_components=n_components, svd_solver='randomized')\n",
    "    feature = pd.DataFrame(Iter.fit_transform(feature))\n",
    "    data = pd.concat([feature, label], axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "# 定义提取 LBP 特征的函数\n",
    "def lbp_feature_extraction(image):\n",
    "    # 将 PIL 图像转换为 NumPy 数组\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    # 计算 LBP 特征\n",
    "    lbp_image = local_binary_pattern(image_array, 8, 1, method='uniform')\n",
    "\n",
    "    # 将 LBP 图像展平成一维向量\n",
    "    lbp_feature = lbp_image.ravel()\n",
    "\n",
    "    return lbp_feature\n",
    "\n",
    "\n",
    "def feature_extraction():\n",
    "    # 初始化特征、标签和攻击类型列表\n",
    "    features = []\n",
    "    labels = []\n",
    "    attack_types = []\n",
    "\n",
    "    # 图像预处理\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(),  # 转为灰度图\n",
    "        transforms.Resize((48, 48)),  # 调整大小为48x48\n",
    "    ])\n",
    "\n",
    "    # 遍历基础文件夹中的 nxxx 文件夹\n",
    "    cnt = 1\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for dir_name in dirs:\n",
    "            # 检查文件夹名称是否符合 nxxx 结构\n",
    "            if dir_name.startswith('n'):\n",
    "                nxxx_folder = os.path.join(root, dir_name)\n",
    "\n",
    "                # 寻找 pcap 文件夹\n",
    "                pcap_folder = os.path.join(nxxx_folder, 'pcap')\n",
    "\n",
    "                if os.path.exists(pcap_folder):\n",
    "                    # 遍历 pcap 文件夹中的 nxxx_xxxx 文件夹\n",
    "                    for pcap_subfolder in os.listdir(pcap_folder):\n",
    "                        subfolder_path = os.path.join(pcap_folder, pcap_subfolder)\n",
    "\n",
    "                        # 确定攻击类型\n",
    "                        attack_type = int(pcap_subfolder.split('_')[-1])  # 提取最后一位数字并转换为整数\n",
    "\n",
    "                        # 寻找 dataset 文件夹中的图片文件\n",
    "                        dataset_folder = os.path.join(subfolder_path, 'dataset')\n",
    "                        benign_folder = os.path.join(dataset_folder, 'benign')\n",
    "                        malicious_folder = os.path.join(dataset_folder, 'malicious')\n",
    "\n",
    "\n",
    "                        # 提取 benign 文件夹中的图片特征并标注数据类型\n",
    "                        if os.path.exists(benign_folder):\n",
    "                            for image_name in os.listdir(benign_folder):\n",
    "                                image_path = os.path.join(benign_folder, image_name)\n",
    "                                image = Image.open(image_path)\n",
    "\n",
    "                                # 图像预处理\n",
    "                                image = transform(image)\n",
    "\n",
    "                                # 使用 LBP 提取特征\n",
    "                                lbp_feature = lbp_feature_extraction(image)\n",
    "\n",
    "                                # 添加特征和标签到列表中\n",
    "                                features.append(lbp_feature.tolist())\n",
    "                                labels.append(0)  # 标注为 0\n",
    "                                attack_types.append(2)\n",
    "\n",
    "                        # 提取 malicious 文件夹中的图片特征并标注数据类型\n",
    "                        if os.path.exists(malicious_folder):\n",
    "                            for image_name in os.listdir(malicious_folder):\n",
    "                                image_path = os.path.join(malicious_folder, image_name)\n",
    "                                image = Image.open(image_path)\n",
    "\n",
    "                                # 图像预处理\n",
    "                                image = transform(image)\n",
    "\n",
    "                                # 使用 LBP 提取特征\n",
    "                                lbp_feature = lbp_feature_extraction(image)\n",
    "\n",
    "                                # 添加特征和标签到列表中\n",
    "                                features.append(lbp_feature.tolist())\n",
    "                                attack_types.append(attack_type)\n",
    "                                labels.append(1)  # 标注为 1\n",
    "\n",
    "                        # 创建包含唯一列名的特征列列表\n",
    "\n",
    "                features = np.array(features)\n",
    "                features.reshape(-1, 2304)\n",
    "                feature_columns = [f'feature_{i}' for i in range(features.shape[1])]\n",
    "\n",
    "                # 将每个特征向量作为单独的列添加到 DataFrame 中\n",
    "                data = pd.DataFrame(features, columns=feature_columns)\n",
    "\n",
    "                # 添加标签和攻击类型列\n",
    "                data['labels'] = labels\n",
    "                data['attack_type'] = attack_types\n",
    "                data.to_csv(directory_path + \"preprocessed/data\" + str(cnt) + \".csv.gz\", index=False,\n",
    "                            compression='gzip')\n",
    "\n",
    "                features = []\n",
    "                labels = []\n",
    "                attack_types = []\n",
    "                data = []\n",
    "\n",
    "    data = glob.glob(directory_path + 'preprocessed/' + '*.gz')\n",
    "    dataframes = []\n",
    "    for file in data:\n",
    "        dataframe = pd.read_csv(file, compression='gzip')\n",
    "        dataframe = pca(dataframe.drop(columns=['labels', 'attack_type']), dataframe[['labels', 'attack_type']])\n",
    "        dataframes.append(dataframe)\n",
    "    data = pd.concat(dataframes, ignore_index=True)\n",
    "    del dataframes\n",
    "\n",
    "    return data\n",
    "\n",
    "    # 将 DataFrame 保存为 gzip 压缩的 CSV 文件\n",
    "    '''if not os.path.exists(directory_path + 'preprocessed'):\n",
    "        os.makedirs(directory_path + 'preprocessed')\n",
    "    data.to_csv(output_file, index=False, compression='gzip')\n",
    "    print(\"saved\")'''\n",
    "\n",
    "\n",
    "# 定义加载数据的函数\n",
    "def load_data():\n",
    "    if os.path.exists(directory_path + \"preprocessed/data.csv.gz\"):\n",
    "        print(\"Loading Preprocessed Data\")\n",
    "        data = pd.read_csv(directory_path + \"preprocessed/data.csv.gz\", compression='gzip')\n",
    "\n",
    "        data.iloc[:, :-3] = data.iloc[:, :-3].astype('float64')\n",
    "        data.iloc[:, -3:] = data.iloc[:, -3:].astype('int')\n",
    "\n",
    "    else:\n",
    "        atk_dict = {1: 'SMB attack', 3: 'SYN flood', 2: 'Benign'}\n",
    "\n",
    "        # data = feature_extraction()\n",
    "        data = glob.glob(directory_path + 'preprocessed/' + '*.gz')\n",
    "        dataframes = []\n",
    "        for file in data:\n",
    "            dataframe = pd.read_csv(file, compression='gzip')\n",
    "            \n",
    "            dataframes.append(dataframe)\n",
    "        data = pd.concat(dataframes, ignore_index=True)\n",
    "        del dataframes\n",
    "        \n",
    "        data = pca(data.drop(columns=['labels', 'attack_type']), data[['labels', 'attack_type']])\n",
    "        \n",
    "        # 调用 data_process 函数对 PCA 后的数据进行处理\n",
    "        data = data_process(data, atk_dict)\n",
    "\n",
    "        if not os.path.exists(directory_path + 'preprocessed'):\n",
    "            os.makedirs(directory_path + 'preprocessed')\n",
    "        data.to_csv(directory_path + \"preprocessed/data.csv.gz\", index=False, compression='gzip')\n",
    "        print(\"saved\")\n",
    "\n",
    "        data.iloc[:, :-3] = data.iloc[:, :-3].astype('float64')\n",
    "        data.iloc[:, -3:] = data.iloc[:, -3:].astype('int')\n",
    "\n",
    "    train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    train_data, val_data = train_data.values, val_data.values\n",
    "\n",
    "    return train_data, val_data\n",
    "\n",
    "def get_data_loader(data, batch_size):\n",
    "    cicids_dataset = CICIDSDataset(data)\n",
    "\n",
    "    return DataLoader(cicids_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiFmfZhu2NCN"
   },
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKUDlHn8FVzo"
   },
   "source": [
    "定义训练与评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "LZAdZpC_FVQl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, r2_score, \\\n",
    "    mean_squared_error, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def plot_metrics(losses, accuracies, roc_auc_scores, confusion_matrix):\n",
    "    # 绘制损失和准确率曲线\n",
    "    epochs = range(1, len(losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, losses, 'r', label='Training Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracies, 'b', label='Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 绘制ROC曲线\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for i in range(len(roc_auc_scores)):\n",
    "        fpr, tpr, _ = roc_auc_scores[i]\n",
    "        plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc_scores[i][-1]:0.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # 绘制混淆矩阵\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def eval_model(model, loader):\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target, _ in loader:\n",
    "            data, target = data.to(\"cuda\"), target.to(\"cuda\")\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target).item()\n",
    "            losses.append(loss)\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "            correct += torch.sum(pred == target).item()\n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            targets.extend(target.cpu().numpy())\n",
    "\n",
    "    eval_loss = np.mean(losses)\n",
    "    eval_acc = correct / len(loader.dataset)\n",
    "\n",
    "    # 计算精确率、召回率、F1 分数\n",
    "    precision = precision_score(targets, predictions, average='weighted')\n",
    "    recall = recall_score(targets, predictions, average='weighted')\n",
    "    f1 = f1_score(targets, predictions, average='weighted')\n",
    "\n",
    "    # 计算 ROC 曲线和 AUC\n",
    "    try:\n",
    "        auc = roc_auc_score(targets, predictions, average='weighted')\n",
    "    except ValueError:\n",
    "        auc = None\n",
    "\n",
    "    # 计算混淆矩阵\n",
    "    conf_matrix = confusion_matrix(targets, predictions)\n",
    "\n",
    "    # 计算 R-squared\n",
    "    r_squared = r2_score(targets, predictions)\n",
    "\n",
    "    # 计算均方误差\n",
    "    mse = mean_squared_error(targets, predictions)\n",
    "\n",
    "    # 计算对数损失\n",
    "    try:\n",
    "        logloss = log_loss(targets, predictions)\n",
    "    except ValueError:\n",
    "        logloss = None\n",
    "\n",
    "    print(\"Loss:\", eval_loss, \"Accuracy:\", eval_acc)\n",
    "    print(\"Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)\n",
    "    print(\"AUC:\", auc)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"R-squared:\", r_squared)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "    print(\"Log Loss:\", logloss)\n",
    "\n",
    "    return eval_loss, eval_acc, precision, recall, f1, auc, conf_matrix, r_squared, mse, logloss\n",
    "\n",
    "\n",
    "def train_model(model, opt, epochs, data, eval_data, path, print_every=100):\n",
    "    model.cuda()\n",
    "\n",
    "    pretrained_path = \"SIDD/pretrained\"\n",
    "    top_acc = 0.\n",
    "\n",
    "    if os.path.exists(pretrained_path + \"/\" + path):\n",
    "        print(\"Loading Pretrained Model\")\n",
    "        state = torch.load(pretrained_path + \"/\" + path)\n",
    "        model.load_state_dict(state[\"model_state_dict\"])\n",
    "        start_epoch = state[\"epoch\"] + 1\n",
    "        losses = state[\"ep_loss\"]\n",
    "        accs = state[\"ep_acc\"]\n",
    "        top_acc = max(accs)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        losses, accs = [], []\n",
    "        try:\n",
    "            os.mkdir(pretrained_path)\n",
    "        except OSError as error:\n",
    "            pass\n",
    "\n",
    "    start = time.time()\n",
    "    temp = start\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(data):\n",
    "            src, trg, _ = batch\n",
    "            src, trg = src.cuda(), trg.cuda()\n",
    "\n",
    "            if isinstance(model, Transformer):\n",
    "                trg_mask = get_mask(128, 8, 25)\n",
    "            else:\n",
    "                trg_mask = None\n",
    "\n",
    "            preds = model(src, trg_mask)\n",
    "            opt.zero_grad()\n",
    "            loss = F.cross_entropy(preds, trg)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.data\n",
    "            if (i + 1) % print_every == 0:\n",
    "                loss_avg = total_loss / print_every\n",
    "                print(\"time = %dm, epoch %d, iter = %d, loss = %.3f, \\\n",
    "                %ds per %d iters\" % ((time.time() - start) // 60,\n",
    "                                     epoch + 1, i + 1, loss_avg, time.time() - temp,\n",
    "                                     print_every))\n",
    "                total_loss = 0\n",
    "                temp = time.time()\n",
    "        eval_loss, eval_acc, precision, recall, f1, auc, conf_matrix, r_squared, mse, logloss = eval_model(model, eval_data)\n",
    "\n",
    "        losses.append(ep_loss)\n",
    "        accs.append(ep_acc)\n",
    "        if ep_acc > top_acc:\n",
    "            top_state = {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(top_state, pretrained_path + \"/max_\" + path)\n",
    "        state = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'ep_loss': losses,\n",
    "            'ep_acc': accs\n",
    "        }\n",
    "        torch.save(state, pretrained_path + \"/\" + path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecvIuqbyFbRW"
   },
   "source": [
    "设置参数并训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YsohpWmFbya"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Preprocessed Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13887/343126135.py:224: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0         1\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         1\n",
      "         ..\n",
      "899995    1\n",
      "899996    1\n",
      "899997    1\n",
      "899998    1\n",
      "899999    1\n",
      "Name: Attack, Length: 900000, dtype: int64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, -3:] = data.iloc[:, -3:].astype('int')\n",
      "/tmp/ipykernel_13887/343126135.py:224: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "         ..\n",
      "899995    0\n",
      "899996    0\n",
      "899997    0\n",
      "899998    0\n",
      "899999    0\n",
      "Name: Benign, Length: 900000, dtype: int64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, -3:] = data.iloc[:, -3:].astype('int')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "          Embedder-1               [-1, 25, 32]               0\n",
      "Positional_Encoder-2               [-1, 25, 32]               0\n",
      "              Norm-3               [-1, 25, 32]              32\n",
      "            Linear-4               [-1, 25, 32]           1,056\n",
      "            Linear-5               [-1, 25, 32]           1,056\n",
      "            Linear-6               [-1, 25, 32]           1,056\n",
      "           Dropout-7            [-1, 8, 25, 25]               0\n",
      "            Linear-8               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-9               [-1, 25, 32]               0\n",
      "          Dropout-10               [-1, 25, 32]               0\n",
      "             Norm-11               [-1, 25, 32]              32\n",
      "           Linear-12             [-1, 25, 1024]          33,792\n",
      "          Dropout-13             [-1, 25, 1024]               0\n",
      "           Linear-14               [-1, 25, 32]          32,800\n",
      "      FeedForward-15               [-1, 25, 32]               0\n",
      "          Dropout-16               [-1, 25, 32]               0\n",
      "    Encoder_Layer-17               [-1, 25, 32]               0\n",
      "             Norm-18               [-1, 25, 32]              32\n",
      "           Linear-19               [-1, 25, 32]           1,056\n",
      "           Linear-20               [-1, 25, 32]           1,056\n",
      "           Linear-21               [-1, 25, 32]           1,056\n",
      "          Dropout-22            [-1, 8, 25, 25]               0\n",
      "           Linear-23               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-24               [-1, 25, 32]               0\n",
      "          Dropout-25               [-1, 25, 32]               0\n",
      "             Norm-26               [-1, 25, 32]              32\n",
      "           Linear-27             [-1, 25, 1024]          33,792\n",
      "          Dropout-28             [-1, 25, 1024]               0\n",
      "           Linear-29               [-1, 25, 32]          32,800\n",
      "      FeedForward-30               [-1, 25, 32]               0\n",
      "          Dropout-31               [-1, 25, 32]               0\n",
      "    Encoder_Layer-32               [-1, 25, 32]               0\n",
      "             Norm-33               [-1, 25, 32]              32\n",
      "           Linear-34               [-1, 25, 32]           1,056\n",
      "           Linear-35               [-1, 25, 32]           1,056\n",
      "           Linear-36               [-1, 25, 32]           1,056\n",
      "          Dropout-37            [-1, 8, 25, 25]               0\n",
      "           Linear-38               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-39               [-1, 25, 32]               0\n",
      "          Dropout-40               [-1, 25, 32]               0\n",
      "             Norm-41               [-1, 25, 32]              32\n",
      "           Linear-42             [-1, 25, 1024]          33,792\n",
      "          Dropout-43             [-1, 25, 1024]               0\n",
      "           Linear-44               [-1, 25, 32]          32,800\n",
      "      FeedForward-45               [-1, 25, 32]               0\n",
      "          Dropout-46               [-1, 25, 32]               0\n",
      "    Encoder_Layer-47               [-1, 25, 32]               0\n",
      "             Norm-48               [-1, 25, 32]              32\n",
      "           Linear-49               [-1, 25, 32]           1,056\n",
      "           Linear-50               [-1, 25, 32]           1,056\n",
      "           Linear-51               [-1, 25, 32]           1,056\n",
      "          Dropout-52            [-1, 8, 25, 25]               0\n",
      "           Linear-53               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-54               [-1, 25, 32]               0\n",
      "          Dropout-55               [-1, 25, 32]               0\n",
      "             Norm-56               [-1, 25, 32]              32\n",
      "           Linear-57             [-1, 25, 1024]          33,792\n",
      "          Dropout-58             [-1, 25, 1024]               0\n",
      "           Linear-59               [-1, 25, 32]          32,800\n",
      "      FeedForward-60               [-1, 25, 32]               0\n",
      "          Dropout-61               [-1, 25, 32]               0\n",
      "    Encoder_Layer-62               [-1, 25, 32]               0\n",
      "             Norm-63               [-1, 25, 32]              32\n",
      "           Linear-64               [-1, 25, 32]           1,056\n",
      "           Linear-65               [-1, 25, 32]           1,056\n",
      "           Linear-66               [-1, 25, 32]           1,056\n",
      "          Dropout-67            [-1, 8, 25, 25]               0\n",
      "           Linear-68               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-69               [-1, 25, 32]               0\n",
      "          Dropout-70               [-1, 25, 32]               0\n",
      "             Norm-71               [-1, 25, 32]              32\n",
      "           Linear-72             [-1, 25, 1024]          33,792\n",
      "          Dropout-73             [-1, 25, 1024]               0\n",
      "           Linear-74               [-1, 25, 32]          32,800\n",
      "      FeedForward-75               [-1, 25, 32]               0\n",
      "          Dropout-76               [-1, 25, 32]               0\n",
      "    Encoder_Layer-77               [-1, 25, 32]               0\n",
      "             Norm-78               [-1, 25, 32]              32\n",
      "           Linear-79               [-1, 25, 32]           1,056\n",
      "           Linear-80               [-1, 25, 32]           1,056\n",
      "           Linear-81               [-1, 25, 32]           1,056\n",
      "          Dropout-82            [-1, 8, 25, 25]               0\n",
      "           Linear-83               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-84               [-1, 25, 32]               0\n",
      "          Dropout-85               [-1, 25, 32]               0\n",
      "             Norm-86               [-1, 25, 32]              32\n",
      "           Linear-87             [-1, 25, 1024]          33,792\n",
      "          Dropout-88             [-1, 25, 1024]               0\n",
      "           Linear-89               [-1, 25, 32]          32,800\n",
      "      FeedForward-90               [-1, 25, 32]               0\n",
      "          Dropout-91               [-1, 25, 32]               0\n",
      "    Encoder_Layer-92               [-1, 25, 32]               0\n",
      "             Norm-93               [-1, 25, 32]              32\n",
      "          Encoder-94               [-1, 25, 32]               0\n",
      "         Embedder-95               [-1, 25, 32]               0\n",
      "Positional_Encoder-96               [-1, 25, 32]               0\n",
      "             Norm-97               [-1, 25, 32]              32\n",
      "           Linear-98               [-1, 25, 32]           1,056\n",
      "           Linear-99               [-1, 25, 32]           1,056\n",
      "          Linear-100               [-1, 25, 32]           1,056\n",
      "         Dropout-101            [-1, 8, 25, 25]               0\n",
      "          Linear-102               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-103               [-1, 25, 32]               0\n",
      "         Dropout-104               [-1, 25, 32]               0\n",
      "            Norm-105               [-1, 25, 32]              32\n",
      "          Linear-106               [-1, 25, 32]           1,056\n",
      "          Linear-107               [-1, 25, 32]           1,056\n",
      "          Linear-108               [-1, 25, 32]           1,056\n",
      "         Dropout-109            [-1, 8, 25, 25]               0\n",
      "          Linear-110               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-111               [-1, 25, 32]               0\n",
      "         Dropout-112               [-1, 25, 32]               0\n",
      "            Norm-113               [-1, 25, 32]              32\n",
      "          Linear-114             [-1, 25, 1024]          33,792\n",
      "         Dropout-115             [-1, 25, 1024]               0\n",
      "          Linear-116               [-1, 25, 32]          32,800\n",
      "     FeedForward-117               [-1, 25, 32]               0\n",
      "         Dropout-118               [-1, 25, 32]               0\n",
      "   Decoder_Layer-119               [-1, 25, 32]               0\n",
      "            Norm-120               [-1, 25, 32]              32\n",
      "          Linear-121               [-1, 25, 32]           1,056\n",
      "          Linear-122               [-1, 25, 32]           1,056\n",
      "          Linear-123               [-1, 25, 32]           1,056\n",
      "         Dropout-124            [-1, 8, 25, 25]               0\n",
      "          Linear-125               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-126               [-1, 25, 32]               0\n",
      "         Dropout-127               [-1, 25, 32]               0\n",
      "            Norm-128               [-1, 25, 32]              32\n",
      "          Linear-129               [-1, 25, 32]           1,056\n",
      "          Linear-130               [-1, 25, 32]           1,056\n",
      "          Linear-131               [-1, 25, 32]           1,056\n",
      "         Dropout-132            [-1, 8, 25, 25]               0\n",
      "          Linear-133               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-134               [-1, 25, 32]               0\n",
      "         Dropout-135               [-1, 25, 32]               0\n",
      "            Norm-136               [-1, 25, 32]              32\n",
      "          Linear-137             [-1, 25, 1024]          33,792\n",
      "         Dropout-138             [-1, 25, 1024]               0\n",
      "          Linear-139               [-1, 25, 32]          32,800\n",
      "     FeedForward-140               [-1, 25, 32]               0\n",
      "         Dropout-141               [-1, 25, 32]               0\n",
      "   Decoder_Layer-142               [-1, 25, 32]               0\n",
      "            Norm-143               [-1, 25, 32]              32\n",
      "          Linear-144               [-1, 25, 32]           1,056\n",
      "          Linear-145               [-1, 25, 32]           1,056\n",
      "          Linear-146               [-1, 25, 32]           1,056\n",
      "         Dropout-147            [-1, 8, 25, 25]               0\n",
      "          Linear-148               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-149               [-1, 25, 32]               0\n",
      "         Dropout-150               [-1, 25, 32]               0\n",
      "            Norm-151               [-1, 25, 32]              32\n",
      "          Linear-152               [-1, 25, 32]           1,056\n",
      "          Linear-153               [-1, 25, 32]           1,056\n",
      "          Linear-154               [-1, 25, 32]           1,056\n",
      "         Dropout-155            [-1, 8, 25, 25]               0\n",
      "          Linear-156               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-157               [-1, 25, 32]               0\n",
      "         Dropout-158               [-1, 25, 32]               0\n",
      "            Norm-159               [-1, 25, 32]              32\n",
      "          Linear-160             [-1, 25, 1024]          33,792\n",
      "         Dropout-161             [-1, 25, 1024]               0\n",
      "          Linear-162               [-1, 25, 32]          32,800\n",
      "     FeedForward-163               [-1, 25, 32]               0\n",
      "         Dropout-164               [-1, 25, 32]               0\n",
      "   Decoder_Layer-165               [-1, 25, 32]               0\n",
      "            Norm-166               [-1, 25, 32]              32\n",
      "          Linear-167               [-1, 25, 32]           1,056\n",
      "          Linear-168               [-1, 25, 32]           1,056\n",
      "          Linear-169               [-1, 25, 32]           1,056\n",
      "         Dropout-170            [-1, 8, 25, 25]               0\n",
      "          Linear-171               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-172               [-1, 25, 32]               0\n",
      "         Dropout-173               [-1, 25, 32]               0\n",
      "            Norm-174               [-1, 25, 32]              32\n",
      "          Linear-175               [-1, 25, 32]           1,056\n",
      "          Linear-176               [-1, 25, 32]           1,056\n",
      "          Linear-177               [-1, 25, 32]           1,056\n",
      "         Dropout-178            [-1, 8, 25, 25]               0\n",
      "          Linear-179               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-180               [-1, 25, 32]               0\n",
      "         Dropout-181               [-1, 25, 32]               0\n",
      "            Norm-182               [-1, 25, 32]              32\n",
      "          Linear-183             [-1, 25, 1024]          33,792\n",
      "         Dropout-184             [-1, 25, 1024]               0\n",
      "          Linear-185               [-1, 25, 32]          32,800\n",
      "     FeedForward-186               [-1, 25, 32]               0\n",
      "         Dropout-187               [-1, 25, 32]               0\n",
      "   Decoder_Layer-188               [-1, 25, 32]               0\n",
      "            Norm-189               [-1, 25, 32]              32\n",
      "          Linear-190               [-1, 25, 32]           1,056\n",
      "          Linear-191               [-1, 25, 32]           1,056\n",
      "          Linear-192               [-1, 25, 32]           1,056\n",
      "         Dropout-193            [-1, 8, 25, 25]               0\n",
      "          Linear-194               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-195               [-1, 25, 32]               0\n",
      "         Dropout-196               [-1, 25, 32]               0\n",
      "            Norm-197               [-1, 25, 32]              32\n",
      "          Linear-198               [-1, 25, 32]           1,056\n",
      "          Linear-199               [-1, 25, 32]           1,056\n",
      "          Linear-200               [-1, 25, 32]           1,056\n",
      "         Dropout-201            [-1, 8, 25, 25]               0\n",
      "          Linear-202               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-203               [-1, 25, 32]               0\n",
      "         Dropout-204               [-1, 25, 32]               0\n",
      "            Norm-205               [-1, 25, 32]              32\n",
      "          Linear-206             [-1, 25, 1024]          33,792\n",
      "         Dropout-207             [-1, 25, 1024]               0\n",
      "          Linear-208               [-1, 25, 32]          32,800\n",
      "     FeedForward-209               [-1, 25, 32]               0\n",
      "         Dropout-210               [-1, 25, 32]               0\n",
      "   Decoder_Layer-211               [-1, 25, 32]               0\n",
      "            Norm-212               [-1, 25, 32]              32\n",
      "          Linear-213               [-1, 25, 32]           1,056\n",
      "          Linear-214               [-1, 25, 32]           1,056\n",
      "          Linear-215               [-1, 25, 32]           1,056\n",
      "         Dropout-216            [-1, 8, 25, 25]               0\n",
      "          Linear-217               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-218               [-1, 25, 32]               0\n",
      "         Dropout-219               [-1, 25, 32]               0\n",
      "            Norm-220               [-1, 25, 32]              32\n",
      "          Linear-221               [-1, 25, 32]           1,056\n",
      "          Linear-222               [-1, 25, 32]           1,056\n",
      "          Linear-223               [-1, 25, 32]           1,056\n",
      "         Dropout-224            [-1, 8, 25, 25]               0\n",
      "          Linear-225               [-1, 25, 32]           1,056\n",
      "Multi_Head_Attention-226               [-1, 25, 32]               0\n",
      "         Dropout-227               [-1, 25, 32]               0\n",
      "            Norm-228               [-1, 25, 32]              32\n",
      "          Linear-229             [-1, 25, 1024]          33,792\n",
      "         Dropout-230             [-1, 25, 1024]               0\n",
      "          Linear-231               [-1, 25, 32]          32,800\n",
      "     FeedForward-232               [-1, 25, 32]               0\n",
      "         Dropout-233               [-1, 25, 32]               0\n",
      "   Decoder_Layer-234               [-1, 25, 32]               0\n",
      "            Norm-235               [-1, 25, 32]              32\n",
      "         Decoder-236               [-1, 25, 32]               0\n",
      "          Linear-237                    [-1, 2]           1,602\n",
      "================================================================\n",
      "Total params: 877,762\n",
      "Trainable params: 876,738\n",
      "Non-trainable params: 1,024\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 6.56\n",
      "Params size (MB): 3.35\n",
      "Estimated Total Size (MB): 9.91\n",
      "----------------------------------------------------------------\n",
      "time = 0m, epoch 1, iter = 100, loss = 0.602,                 5s per 100 iters\n",
      "time = 0m, epoch 1, iter = 200, loss = 0.549,                 5s per 100 iters\n",
      "time = 0m, epoch 1, iter = 300, loss = 0.544,                 5s per 100 iters\n",
      "time = 0m, epoch 1, iter = 400, loss = 0.546,                 5s per 100 iters\n",
      "time = 0m, epoch 1, iter = 500, loss = 0.542,                 5s per 100 iters\n",
      "time = 0m, epoch 1, iter = 600, loss = 0.534,                 5s per 100 iters\n",
      "time = 0m, epoch 1, iter = 700, loss = 0.535,                 5s per 100 iters\n",
      "time = 0m, epoch 1, iter = 800, loss = 0.535,                 5s per 100 iters\n",
      "time = 0m, epoch 1, iter = 900, loss = 0.541,                 5s per 100 iters\n",
      "time = 0m, epoch 1, iter = 1000, loss = 0.539,                 5s per 100 iters\n",
      "time = 0m, epoch 1, iter = 1100, loss = 0.535,                 5s per 100 iters\n",
      "time = 1m, epoch 1, iter = 1200, loss = 0.534,                 5s per 100 iters\n",
      "time = 1m, epoch 1, iter = 1300, loss = 0.535,                 5s per 100 iters\n",
      "time = 1m, epoch 1, iter = 1400, loss = 0.533,                 5s per 100 iters\n",
      "time = 1m, epoch 1, iter = 1500, loss = 0.535,                 5s per 100 iters\n",
      "time = 1m, epoch 1, iter = 1600, loss = 0.540,                 5s per 100 iters\n",
      "time = 1m, epoch 1, iter = 1700, loss = 0.539,                 5s per 100 iters\n",
      "time = 1m, epoch 1, iter = 1800, loss = 0.539,                 5s per 100 iters\n",
      "time = 1m, epoch 1, iter = 1900, loss = 0.535,                 5s per 100 iters\n",
      "time = 1m, epoch 1, iter = 2000, loss = 0.542,                 5s per 100 iters\n",
      "time = 1m, epoch 1, iter = 2100, loss = 0.536,                 5s per 100 iters\n",
      "time = 1m, epoch 1, iter = 2200, loss = 0.538,                 5s per 100 iters\n",
      "time = 2m, epoch 1, iter = 2300, loss = 0.534,                 5s per 100 iters\n",
      "time = 2m, epoch 1, iter = 2400, loss = 0.539,                 5s per 100 iters\n",
      "time = 2m, epoch 1, iter = 2500, loss = 0.536,                 5s per 100 iters\n",
      "time = 2m, epoch 1, iter = 2600, loss = 0.537,                 5s per 100 iters\n",
      "time = 2m, epoch 1, iter = 2700, loss = 0.535,                 5s per 100 iters\n",
      "time = 2m, epoch 1, iter = 2800, loss = 0.534,                 5s per 100 iters\n",
      "time = 2m, epoch 1, iter = 2900, loss = 0.538,                 5s per 100 iters\n",
      "time = 2m, epoch 1, iter = 3000, loss = 0.536,                 5s per 100 iters\n",
      "time = 2m, epoch 1, iter = 3100, loss = 0.537,                 5s per 100 iters\n",
      "time = 2m, epoch 1, iter = 3200, loss = 0.536,                 5s per 100 iters\n",
      "time = 2m, epoch 1, iter = 3300, loss = 0.538,                 5s per 100 iters\n",
      "time = 2m, epoch 1, iter = 3400, loss = 0.534,                 5s per 100 iters\n",
      "time = 3m, epoch 1, iter = 3500, loss = 0.538,                 5s per 100 iters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "\n",
    "def main():\n",
    "    learning_rate = 5e-4\n",
    "    batch_size = 128\n",
    "    epochs = 30\n",
    "    dropout_rate = 0.5\n",
    "    d_model = 32\n",
    "    heads = 8\n",
    "    N = 6\n",
    "    trg_vocab = 2\n",
    "\n",
    "    train_data, val_data = load_data()\n",
    "\n",
    "    train_loader = get_data_loader(train_data, batch_size)\n",
    "    val_loader = get_data_loader(val_data, batch_size)\n",
    "\n",
    "    model = Transformer(trg_vocab, d_model, N, heads, dropout_rate)\n",
    "    save_path = \"pretrained.pt\"\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    model = model.to(\"cuda:0\")\n",
    "    summary(model, (25, ))\n",
    "\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_model(model, optim, epochs, train_loader, val_loader, save_path)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP+IHTbIkOXjCwVizTZAs8t",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
